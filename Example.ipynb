{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28621a1e",
   "metadata": {},
   "source": [
    "# Example of the usage of the Weak label classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d4dcfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f29cb012",
   "metadata": {},
   "source": [
    "We first need to load:\n",
    "\n",
    "1. **Standard Python libraries** for data handling and reproducibility.  \n",
    "2. **PyTorch** (and its submodules) for model definition, training, and data loading.  \n",
    "3. **Custom modules** from this project:\n",
    "   - **`train_test_loop`**: provides the `train_and_evaluate` function to run training and evaluation loops.  \n",
    "   - **`losses`**: contains various weak‐label‐aware loss functions like `FwdBwdLoss`.  \n",
    "   - **`weakener`**: implements the `Weakener` class for generating noisy/weak labels.  \n",
    "   - **`model`**: defines model architectures .\n",
    "   - **`dataset`**: provides `Data_handling` (and other dataset classes) for loading and splitting data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f62fdf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# PyTorch core\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Custom project modules\n",
    "from utils.train_test_loop import train_and_evaluate\n",
    "from utils.losses import FwdLoss, EMLoss, FwdBwdLoss, MarginalChainLoss\n",
    "from utils.losses1 import MarginalChainProperLoss, ForwardProperLoss, scoring_matrix\n",
    "from utils.losses1 import UpperBoundWeakProperLoss\n",
    "from utils.dataset_visualization import visualize_dataset\n",
    "from src.weakener import Weakener\n",
    "from src.model import MLP\n",
    "from src.dataset import Data_handling\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68db405b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73f12c25",
   "metadata": {},
   "source": [
    "## Loading and Visualizing Iris\n",
    "\n",
    "1. **Instantiate** our `Data_handling` class to load the Iris dataset from OpenML (ID 61) using an 80/20 train/test split.  \n",
    "2. **Retrieve** the raw arrays of features and labels via `get_data()`.  \n",
    "3. **Combine** the train and test portions back into a single DataFrame \n",
    "4. **Visualize** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cb6a1f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e44cfc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m dataset_name = \u001b[33m'\u001b[39m\u001b[33mCifar10\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m Data = \u001b[43mData_handling\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# dataset='mnist',\u001b[39;49;00m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshuffling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplitting_seed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Weak_MC-1/src/dataset.py:171\u001b[39m, in \u001b[36mData_handling.__init__\u001b[39m\u001b[34m(self, dataset, train_size, test_size, valid_size, batch_size, shuffling, splitting_seed)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;28mself\u001b[39m.transform = transforms.Compose([\n\u001b[32m    161\u001b[39m     transforms.RandomCrop(\u001b[32m32\u001b[39m, padding=\u001b[32m4\u001b[39m),\n\u001b[32m    162\u001b[39m     transforms.RandomHorizontalFlip(),\n\u001b[32m    163\u001b[39m     transforms.ToTensor(),\n\u001b[32m    164\u001b[39m     transforms.Normalize((\u001b[32m0.5\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m0.5\u001b[39m), (\u001b[32m0.5\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m0.5\u001b[39m))\n\u001b[32m    165\u001b[39m     ])\n\u001b[32m    166\u001b[39m \u001b[38;5;28mself\u001b[39m.train_dataset = datasets.\u001b[34m__dict__\u001b[39m[\u001b[38;5;28mself\u001b[39m.dataset](\n\u001b[32m    167\u001b[39m     root=\u001b[33m'\u001b[39m\u001b[33mDatasets/raw_datasets\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m    168\u001b[39m     train=\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[32m    169\u001b[39m     transform=\u001b[38;5;28mself\u001b[39m.transform, \n\u001b[32m    170\u001b[39m     download=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m \u001b[38;5;28mself\u001b[39m.test_dataset = \u001b[43mdatasets\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__dict__\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mDatasets/raw_datasets\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28mself\u001b[39m.num_classes = \u001b[38;5;28mlen\u001b[39m(np.unique(\u001b[38;5;28mself\u001b[39m.train_dataset.targets))\n\u001b[32m    180\u001b[39m \u001b[38;5;28mself\u001b[39m.train_num_samples = \u001b[38;5;28mself\u001b[39m.train_dataset.data.shape[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Weak_MC/.venv/lib/python3.12/site-packages/torchvision/datasets/cifar.py:66\u001b[39m, in \u001b[36mCIFAR10.__init__\u001b[39m\u001b[34m(self, root, train, transform, target_transform, download)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28mself\u001b[39m.train = train  \u001b[38;5;66;03m# training set or test set\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_integrity():\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Weak_MC/.venv/lib/python3.12/site-packages/torchvision/datasets/cifar.py:137\u001b[39m, in \u001b[36mCIFAR10.download\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdownload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_integrity\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    139\u001b[39m     download_and_extract_archive(\u001b[38;5;28mself\u001b[39m.url, \u001b[38;5;28mself\u001b[39m.root, filename=\u001b[38;5;28mself\u001b[39m.filename, md5=\u001b[38;5;28mself\u001b[39m.tgz_md5)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Weak_MC/.venv/lib/python3.12/site-packages/torchvision/datasets/cifar.py:132\u001b[39m, in \u001b[36mCIFAR10._check_integrity\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m filename, md5 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.train_list + \u001b[38;5;28mself\u001b[39m.test_list:\n\u001b[32m    131\u001b[39m     fpath = os.path.join(\u001b[38;5;28mself\u001b[39m.root, \u001b[38;5;28mself\u001b[39m.base_folder, filename)\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcheck_integrity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    133\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Weak_MC/.venv/lib/python3.12/site-packages/torchvision/datasets/utils.py:58\u001b[39m, in \u001b[36mcheck_integrity\u001b[39m\u001b[34m(fpath, md5)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m md5 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcheck_md5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Weak_MC/.venv/lib/python3.12/site-packages/torchvision/datasets/utils.py:50\u001b[39m, in \u001b[36mcheck_md5\u001b[39m\u001b[34m(fpath, md5, **kwargs)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_md5\u001b[39m(fpath: Union[\u001b[38;5;28mstr\u001b[39m, pathlib.Path], md5: \u001b[38;5;28mstr\u001b[39m, **kwargs: Any) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m md5 == \u001b[43mcalculate_md5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Weak_MC/.venv/lib/python3.12/site-packages/torchvision/datasets/utils.py:44\u001b[39m, in \u001b[36mcalculate_md5\u001b[39m\u001b[34m(fpath, chunk_size)\u001b[39m\n\u001b[32m     42\u001b[39m     md5 = hashlib.md5()\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(fpath, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m chunk := \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     45\u001b[39m         md5.update(chunk)\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m md5.hexdigest()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "dataset_name = 'Cifar10'\n",
    "Data = Data_handling(\n",
    "    # dataset='mnist',\n",
    "    dataset=dataset_name,\n",
    "    train_size=0.8,\n",
    "    test_size=0.2,\n",
    "    batch_size=64,\n",
    "    shuffling=False,\n",
    "    splitting_seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655ebde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input. shape=(50000, 3, 32, 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m Data.train_dataset.targets \u001b[38;5;66;03m# This is Train_y\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(Data.test_dataset.targets)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mData\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfeature_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mData\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m] = [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m Data.train_dataset.targets.numpy()]\n\u001b[32m      6\u001b[39m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Weak_MC/.venv/lib/python3.12/site-packages/pandas/core/frame.py:827\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    816\u001b[39m         mgr = dict_to_mgr(\n\u001b[32m    817\u001b[39m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[32m    818\u001b[39m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    824\u001b[39m             copy=_copy,\n\u001b[32m    825\u001b[39m         )\n\u001b[32m    826\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m827\u001b[39m         mgr = \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[32m    837\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Weak_MC/.venv/lib/python3.12/site-packages/pandas/core/internals/construction.py:314\u001b[39m, in \u001b[36mndarray_to_mgr\u001b[39m\u001b[34m(values, index, columns, dtype, copy, typ)\u001b[39m\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    313\u001b[39m         values = np.asarray(values)\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     values = \u001b[43m_ensure_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    317\u001b[39m     \u001b[38;5;66;03m# by definition an array here\u001b[39;00m\n\u001b[32m    318\u001b[39m     \u001b[38;5;66;03m# the dtypes will be coerced to a single dtype\u001b[39;00m\n\u001b[32m    319\u001b[39m     values = _prep_ndarraylike(values, copy=copy_on_sanitize)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Weak_MC/.venv/lib/python3.12/site-packages/pandas/core/internals/construction.py:592\u001b[39m, in \u001b[36m_ensure_2d\u001b[39m\u001b[34m(values)\u001b[39m\n\u001b[32m    590\u001b[39m     values = values.reshape((values.shape[\u001b[32m0\u001b[39m], \u001b[32m1\u001b[39m))\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m values.ndim != \u001b[32m2\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m592\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMust pass 2-d input. shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    593\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "\u001b[31mValueError\u001b[39m: Must pass 2-d input. shape=(50000, 3, 32, 32)"
     ]
    }
   ],
   "source": [
    "Data.train_dataset.data # This is Train_X\n",
    "Data.train_dataset.targets # This is Train_y\n",
    "print(Data.test_dataset.targets)\n",
    "df = pd.DataFrame(Data.train_dataset.data.numpy(), columns=[f'feature_{i}' for i in range(Data.train_dataset.data.shape[1])])\n",
    "df['target'] = [i for i in Data.train_dataset.targets.numpy()]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b001a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e126947",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" df_2_plot = df.iloc[0:1000]\n",
    "features = ['feature_102', 'feature_103']\n",
    "visualize_dataset(\n",
    "    df_2_plot,\n",
    "    features=features,\n",
    "    classes=Data.num_classes,\n",
    "    title=dataset_name,\n",
    ") \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ce7928",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" df_2_plot[[features[0], features[1]]] \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27509ea8",
   "metadata": {},
   "source": [
    "Next, we’ll simulate a **partial‐label learning** or **noisy-label** setting by corrupting each true label with **M**:\n",
    "\n",
    "1. **Instantiate** a `Weakener` with the number of true classes.  \n",
    "2. **Build** a mixing matrix via `generate_M(model_class='pll', corr_p=…)` \n",
    "3. **Generate** weak labels with `generate_weak`, which returns:\n",
    "   - `z`: the integer index of the weak‐label   \n",
    "   - `w`: a binary matrix of shape `(n_samples, n_classes)` indicating the candidate labels  \n",
    "4. **Insert** the partial labels into our Data using `include_weak(w)`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2587776a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated M matrix:\n",
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 1.34217728e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  1.34217728e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  3.35544320e-02 3.35544320e-02]\n",
      " ...\n",
      " [2.04800000e-06 2.04800000e-06 2.04800000e-06 ... 2.04800000e-06\n",
      "  0.00000000e+00 2.04800000e-06]\n",
      " [2.04800000e-06 2.04800000e-06 2.04800000e-06 ... 2.04800000e-06\n",
      "  2.04800000e-06 0.00000000e+00]\n",
      " [5.12000000e-07 5.12000000e-07 5.12000000e-07 ... 5.12000000e-07\n",
      "  5.12000000e-07 5.12000000e-07]]\n",
      "Generated z (noisy labels):\n",
      "tensor([136, 274, 162,  ...,   8, 767, 320], dtype=torch.int32)\n",
      "Inputs batch shape: torch.Size([64, 3, 32, 32])\n",
      "Weak (partial) labels shape: torch.Size([64])\n",
      "True one-hot labels shape: torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "corr_p = 0.2\n",
    "weakener = Weakener(true_classes=Data.num_classes)\n",
    "weakener.generate_M(model_class='pll', corr_p=0.2)\n",
    "# weakener.generate_M(model_class='unif_noise', corr_p=0.5) #Try this for noisy labels\n",
    "print(f\"Generated M matrix:\\n{weakener.M}\")\n",
    "true_onehot = Data.train_dataset.targets  # shape: (n_samples, n_classes)\n",
    "\n",
    "z = weakener.generate_weak(true_onehot)\n",
    "print(f\"Generated z (noisy labels):\\n{z}\")\n",
    "#print(f\"Generated w (multi-label matrix):\\n{w}\")\n",
    "\n",
    "Data.include_weak(z)\n",
    "\n",
    "train_loader, test_loader = Data.get_dataloader(weak_labels='weak')\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "xb, wb, yb = batch\n",
    "print(f\"Inputs batch shape: {xb.shape}\")\n",
    "print(f\"Weak (partial) labels shape: {wb.shape}\")\n",
    "print(f\"True one-hot labels shape: {yb.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a69c4f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>weak_0</th>\n",
       "      <th>weak_1</th>\n",
       "      <th>weak_2</th>\n",
       "      <th>weak_3</th>\n",
       "      <th>weak_4</th>\n",
       "      <th>weak_5</th>\n",
       "      <th>weak_6</th>\n",
       "      <th>weak_7</th>\n",
       "      <th>weak_8</th>\n",
       "      <th>weak_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.537255</td>\n",
       "      <td>-0.662745</td>\n",
       "      <td>-0.607843</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.231373</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>0.090196</td>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.168628</td>\n",
       "      <td>0.168628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.207843</td>\n",
       "      <td>-0.011765</td>\n",
       "      <td>-0.176471</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>-0.019608</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.349020</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.113726</td>\n",
       "      <td>-0.129412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.780392</td>\n",
       "      <td>-0.709804</td>\n",
       "      <td>-0.701961</td>\n",
       "      <td>-0.670588</td>\n",
       "      <td>-0.654902</td>\n",
       "      <td>-0.686275</td>\n",
       "      <td>-0.686275</td>\n",
       "      <td>-0.811765</td>\n",
       "      <td>-0.749020</td>\n",
       "      <td>-0.662745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.317647</td>\n",
       "      <td>0.388235</td>\n",
       "      <td>0.435294</td>\n",
       "      <td>0.419608</td>\n",
       "      <td>0.388235</td>\n",
       "      <td>0.419608</td>\n",
       "      <td>0.443137</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.184314</td>\n",
       "      <td>0.223529</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.231373</td>\n",
       "      <td>0.262745</td>\n",
       "      <td>0.262745</td>\n",
       "      <td>0.262745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>-0.725490</td>\n",
       "      <td>-0.686275</td>\n",
       "      <td>-0.670588</td>\n",
       "      <td>-0.694118</td>\n",
       "      <td>-0.662745</td>\n",
       "      <td>-0.647059</td>\n",
       "      <td>-0.623529</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>-0.584314</td>\n",
       "      <td>-0.560784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>0.482353</td>\n",
       "      <td>0.458824</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.443137</td>\n",
       "      <td>0.427451</td>\n",
       "      <td>0.419608</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.419608</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>0.796078</td>\n",
       "      <td>0.850980</td>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.850980</td>\n",
       "      <td>0.898039</td>\n",
       "      <td>0.929412</td>\n",
       "      <td>0.929412</td>\n",
       "      <td>0.898039</td>\n",
       "      <td>0.850980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 3083 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0      -0.537255  -0.662745  -0.607843  -0.466667  -0.231373  -0.066667   \n",
       "1       0.207843  -0.011765  -0.176471  -0.200000  -0.019608   0.215686   \n",
       "2       1.000000   0.984314   0.984314   0.984314   0.984314   0.984314   \n",
       "3      -0.780392  -0.709804  -0.701961  -0.670588  -0.654902  -0.686275   \n",
       "4       0.333333   0.317647   0.388235   0.435294   0.419608   0.388235   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "49995   0.137255   0.184314   0.223529   0.215686   0.200000   0.200000   \n",
       "49996   1.000000   0.992157   1.000000   0.992157   0.992157   0.992157   \n",
       "49997  -0.725490  -0.686275  -0.670588  -0.694118  -0.662745  -0.647059   \n",
       "49998   0.482353   0.458824   0.450980   0.443137   0.427451   0.419608   \n",
       "49999   0.796078   0.850980   0.835294   0.811765   0.850980   0.898039   \n",
       "\n",
       "       feature_6  feature_7  feature_8  feature_9  ...  weak_0  weak_1  \\\n",
       "0       0.090196   0.137255   0.168628   0.168628  ...     0.0     0.0   \n",
       "1       0.349020   0.411765   0.113726  -0.129412  ...     0.0     1.0   \n",
       "2       0.984314   0.984314   0.984314   0.984314  ...     0.0     0.0   \n",
       "3      -0.686275  -0.811765  -0.749020  -0.662745  ...     0.0     0.0   \n",
       "4       0.419608   0.443137   0.482353   0.482353  ...     0.0     1.0   \n",
       "...          ...        ...        ...        ...  ...     ...     ...   \n",
       "49995   0.231373   0.262745   0.262745   0.262745  ...     0.0     0.0   \n",
       "49996   0.992157   0.984314   0.992157   0.992157  ...     0.0     1.0   \n",
       "49997  -0.623529  -0.600000  -0.584314  -0.560784  ...     0.0     0.0   \n",
       "49998   0.411765   0.411765   0.419608   0.411765  ...     1.0     1.0   \n",
       "49999   0.929412   0.929412   0.898039   0.850980  ...     0.0     1.0   \n",
       "\n",
       "       weak_2  weak_3  weak_4  weak_5  weak_6  weak_7  weak_8  weak_9  \n",
       "0         1.0     0.0     0.0     0.0     1.0     0.0     0.0     1.0  \n",
       "1         0.0     0.0     0.0     1.0     0.0     0.0     1.0     1.0  \n",
       "2         1.0     0.0     1.0     0.0     0.0     0.0     1.0     1.0  \n",
       "3         1.0     0.0     1.0     0.0     0.0     0.0     0.0     0.0  \n",
       "4         0.0     0.0     1.0     0.0     0.0     0.0     0.0     0.0  \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...  \n",
       "49995     1.0     1.0     0.0     0.0     0.0     0.0     0.0     1.0  \n",
       "49996     0.0     0.0     0.0     0.0     1.0     1.0     0.0     0.0  \n",
       "49997     0.0     0.0     0.0     0.0     1.0     0.0     0.0     1.0  \n",
       "49998     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0  \n",
       "49999     0.0     1.0     0.0     0.0     0.0     0.0     0.0     1.0  \n",
       "\n",
       "[50000 rows x 3083 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#weak_df = pd.DataFrame(Data.train_dataset.data.numpy(), columns=[f'feature_{i}' for i in range(Data.train_dataset.data.shape[1])])\n",
    "#df['target'] = [i for i in weakener.w.numpy()]\n",
    "#df\n",
    "\n",
    "# 1) 展平成 (N, 3072)\n",
    "X = Data.train_dataset.data                # (N, 3, 32, 32)  (torch tensor)\n",
    "X2 = X.view(X.shape[0], -1).cpu().numpy()  # (N, 3072)\n",
    "\n",
    "weak_df = pd.DataFrame(X2, columns=[f'feature_{i}' for i in range(X2.shape[1])])\n",
    "\n",
    "# 2) 加 true label（如果 targets 是 one-hot，就转成 class index）\n",
    "y = Data.train_dataset.targets\n",
    "if hasattr(y, \"ndim\") and y.ndim == 2:\n",
    "    y = y.argmax(dim=1)\n",
    "weak_df[\"target\"] = y.cpu().numpy()\n",
    "\n",
    "# 3) 加 weak label（weakener.w 可能是一维或二维：做个兼容）\n",
    "w = weakener.w\n",
    "w_np = w.detach().cpu().numpy()\n",
    "\n",
    "if w_np.ndim == 1:\n",
    "    weak_df[\"weak\"] = w_np\n",
    "else:\n",
    "    # 如果是 one-hot / multi-hot (N,C)，你可以：\n",
    "    # A) 每一类一列（适合做统计）\n",
    "    for c in range(w_np.shape[1]):\n",
    "        weak_df[f\"weak_{c}\"] = w_np[:, c]\n",
    "    # 或 B) 压缩成“候选集合”（适合阅读）\n",
    "    # weak_df[\"weak_set\"] = [np.flatnonzero(row).tolist() for row in w_np]\n",
    "\n",
    "weak_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7430f462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_dataset(\n",
    "#     df,\n",
    "#     features=['feature_0', 'feature_1'],\n",
    "#     classes=3,\n",
    "#     title='Iris Samples with Pie Markers for Multi-Label Entries'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a53b540",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **Instantiate** the model (e.g. `MLP`) with its input/output dimensions.   \n",
    "2. **Choose** the optimizer and set hyperparameters.  \n",
    "3. **Define** the loss function.\n",
    "\n",
    "We also could do a learning rate scheduler (e.g. `StepLR`) to decrease the LR over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f8ac2a",
   "metadata": {},
   "source": [
    "## Training the MLP (using `train_test_loop.py`)\n",
    "\n",
    "1. **Set** training hyperparameters  \n",
    "2. **Call** `train_and_evaluate(model, train_loader, test_loader, optimizer, pll_loss, num_epochs, corr_p)`\n",
    "3. **Plot** results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fac0280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "   epoch  train_loss  train_acc  test_acc  train_detached_loss  \\\n",
      "0      1    0.091608    0.17134    0.2135             0.034012   \n",
      "\n",
      "   test_detached_loss optimizer loss_fn repetition  initial_lr  actual_lr  \\\n",
      "0            0.034162      Adam    None       None    0.000001   0.000001   \n",
      "\n",
      "   corr_p   epoch_time  \n",
      "0     0.2  1325.941068  \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "def resnet18_cifar(num_classes: int):\n",
    "    m = models.resnet18(weights=None)\n",
    "    # CIFAR10: 32x32，改第一层卷积 + 去掉 maxpool\n",
    "    m.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    m.maxpool = nn.Identity()\n",
    "    m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
    "    return m\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = resnet18_cifar(Data.num_classes).to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.1,\n",
    "    momentum=0.9,\n",
    "    weight_decay=5e-4\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 0) 先从 loader 拿一个 batch 推断 input_dim\n",
    "\"\"\" xb, wb, yb = next(iter(train_loader))\n",
    "input_dim = int(np.prod(xb.shape[1:]))   # CIFAR10 通常是 3*32*32=3072\n",
    "\n",
    "print(\"xb shape:\", xb.shape, \"=> input_dim:\", input_dim)\n",
    "\n",
    "# 1) 模型：先 Flatten 再 MLP（这样即使 xb 是 4D 也能喂给 MLP）\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(start_dim=1),\n",
    "    MLP(\n",
    "        input_size=input_dim,\n",
    "        hidden_sizes=[],\n",
    "        output_size=Data.num_classes,\n",
    "        dropout_p=0,\n",
    "        bn=False,\n",
    "        activation='relu'\n",
    "    )\n",
    ")\n",
    " \"\"\"\n",
    "\n",
    "\"\"\" model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ") \"\"\"\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "\n",
    "#optimizer = optim.SGD(\n",
    "#    model.parameters(),\n",
    "#    lr=0.01,\n",
    "#    momentum=0.0  # 0.9\n",
    "#)\n",
    "\n",
    "# 2. Training parameters\n",
    "num_epochs = 1\n",
    "\n",
    "em_loss = MarginalChainProperLoss(weakener.M, loss_code=\"cross_entropy\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ad3c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getenv(\"PYTHONBREAKPOINT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89db1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "\n",
    "\n",
    "# 2. Training parameters\n",
    "num_epochs = 60\n",
    "\n",
    "#em_loss = FwdLoss(weakener.M)\n",
    "em_loss = ForwardProperLoss(weakener.M, loss_code=\"cross_entropy\")\n",
    "#em_loss = MarginalChainProperLoss(weakener.M, loss_code=\"cross_entropy\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910d1634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 1. 固定随机种子\n",
    "torch.manual_seed(0)\n",
    "\n",
    "B, C = 5, 4\n",
    "\n",
    "logits = torch.randn(B, C, requires_grad=True)\n",
    "z = torch.randint(0, C, (B,))\n",
    "\n",
    "M = torch.rand(C, C)\n",
    "M = M / M.sum(dim=1, keepdim=True)\n",
    "F = M.clone()\n",
    "\n",
    "# 这里贴上你的 MarginalChainProperLoss 和 ForwardProperLoss 定义\n",
    "# loss_code=\"cross_entropy\"\n",
    "\n",
    "mc_loss_fn = MarginalChainProperLoss(M, loss_code=\"cross_entropy\", reduction=\"mean\")\n",
    "fw_loss_fn = ForwardProperLoss(F, loss_code=\"cross_entropy\", reduction=\"mean\")\n",
    "\n",
    "# Marginal chain\n",
    "logits_mc = logits.clone().detach().requires_grad_(True)\n",
    "loss_mc = mc_loss_fn(logits_mc, z)\n",
    "loss_mc.backward()\n",
    "grad_mc = logits_mc.grad.clone().detach()\n",
    "\n",
    "# Forward\n",
    "logits_fw = logits.clone().detach().requires_grad_(True)\n",
    "loss_fw = fw_loss_fn(logits_fw, z)\n",
    "loss_fw.backward()\n",
    "grad_fw = logits_fw.grad.clone().detach()\n",
    "\n",
    "print(\"loss_mc:\", loss_mc.item())\n",
    "print(\"loss_fw:\", loss_fw.item())\n",
    "print(\"loss diff:\", abs(loss_mc.item() - loss_fw.item()))\n",
    "\n",
    "print(\"grad same?\", torch.allclose(grad_mc, grad_fw, atol=1e-6))\n",
    "print(\"grad max diff:\", (grad_mc - grad_fw).abs().max().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c50f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取一个 batch\n",
    "xb, zb, yb = next(iter(train_loader))   # 确保 zb 就是 z（weak index）\n",
    "xb = xb.to(device)\n",
    "zb = zb.to(device)\n",
    "\n",
    "logits = model(xb)\n",
    "\n",
    "fwd_loss_fn = ForwardProperLoss(weakener.M, \"cross_entropy\").to(device)\n",
    "ub_loss_fn  = UpperBoundWeakProperLoss(weakener.M, \"cross_entropy\").to(device)\n",
    "\n",
    "loss_fwd = fwd_loss_fn(logits, zb)\n",
    "loss_ub  = ub_loss_fn(logits, zb)\n",
    "\n",
    "print(\"loss_fwd:\", loss_fwd.item())\n",
    "print(\"loss_ub :\", loss_ub.item())\n",
    "\n",
    "g1 = torch.autograd.grad(loss_fwd, logits, retain_graph=True)[0]\n",
    "g2 = torch.autograd.grad(loss_ub,  logits)[0]\n",
    "print(\"grad norm fwd:\", g1.norm().item())\n",
    "print(\"grad norm ub :\",  g2.norm().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0822459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "\n",
    "\n",
    "# 2. Training parameters\n",
    "num_epochs = 60\n",
    "\n",
    "#em_loss = FwdLoss(weakener.M)\n",
    "em_loss = UpperBoundWeakProperLoss(weakener.M, loss_code=\"cross_entropy\")\n",
    "#em_loss = MarginalChainProperLoss(weakener.M, loss_code=\"cross_entropy\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d20a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "# 2. Training parameters\n",
    "num_epochs = 60\n",
    "\n",
    "em_loss = MarginalChainProperLoss(weakener.M, loss_code=\"ps_2\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31519ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.015,\n",
    ")\n",
    "# 2. Training parameters\n",
    "num_epochs = 90\n",
    "\n",
    "em_loss = ForwardProperLoss(weakener.M, loss_code=\"ps_2\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d1d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "# 2. Training parameters\n",
    "num_epochs = 60\n",
    "\n",
    "em_loss = UpperBoundWeakProperLoss(weakener.M, loss_code=\"ps_2\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34351956",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.015,\n",
    ")\n",
    "# 2. Training parameters\n",
    "num_epochs = 400\n",
    "\n",
    "em_loss = MarginalChainProperLoss(weakener.M, loss_code=\"tsallis_0.2\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f39e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "# 2. Training parameters\n",
    "num_epochs = 60\n",
    "\n",
    "em_loss = ForwardProperLoss(weakener.M, loss_code=\"tsallis_0.5\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3583bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "# 2. Training parameters\n",
    "num_epochs = 60\n",
    "\n",
    "em_loss = UpperBoundWeakProperLoss(weakener.M, loss_code=\"tsallis_0.2\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3350bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Weak Train Loss')\n",
    "ax1.plot(clean_results['epoch'], clean_results['train_loss'], label='Supervised Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Weak Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Weak Test Accuracy')\n",
    "ax2.plot(clean_results['epoch'], clean_results['train_acc'],'--', label='Supervised Train Accuracy' )\n",
    "ax2.plot(clean_results['epoch'], clean_results['test_acc'], '--', label='Supervied Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfceed9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
