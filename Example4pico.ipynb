{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28621a1e",
   "metadata": {},
   "source": [
    "# Example of the usage of the Weak label classifier\n",
    "\n",
    "We first need to load:\n",
    "\n",
    "1. **Standard Python libraries** for data handling and reproducibility.  \n",
    "2. **PyTorch** (and its submodules) for model definition, training, and data loading.  \n",
    "3. **Custom modules** from this project:\n",
    "   - **`train_test_loop`**: provides the `train_and_evaluate` function to run training and evaluation loops.  \n",
    "   - **`losses`**: contains various weak‐label‐aware loss functions like `FwdBwdLoss`.  \n",
    "   - **`weakener`**: implements the `Weakener` class for generating noisy/weak labels.  \n",
    "   - **`model`**: defines model architectures .\n",
    "   - **`dataset`**: provides `Data_handling` (and other dataset classes) for loading and splitting data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62fdf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "# import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch core\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Custom project modules\n",
    "from utils.train_test_loop import train_and_evaluate\n",
    "# from utils.losses import FwdLoss, EMLoss, FwdBwdLoss, MarginalChainLoss\n",
    "from utils.losses1 import MarginalChainProperLoss, ForwardProperLoss, scoring_matrix\n",
    "from utils.losses1 import PiCOLoss, IRLoss, UpperBoundWeakProperLoss\n",
    "# from utils.dataset_visualization import visualize_dataset\n",
    "from src.weakener import Weakener\n",
    "from src.model import MLP\n",
    "from src.dataset import Data_handling\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f12c25",
   "metadata": {},
   "source": [
    "## Loading and Visualizing Iris\n",
    "\n",
    "1. **Instantiate** our `Data_handling` class to load the Iris dataset from OpenML (ID 61) using an 80/20 train/test split.  \n",
    "2. **Retrieve** the raw arrays of features and labels via `get_data()`.  \n",
    "3. **Combine** the train and test portions back into a single DataFrame \n",
    "4. **Visualize** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e44cfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = 'Cifar10'\n",
    "dataset_name = 'mnist'\n",
    "Data = Data_handling(\n",
    "    dataset=dataset_name,\n",
    "    train_size=0.8,\n",
    "    test_size=0.2,\n",
    "    batch_size=64,\n",
    "    shuffling=False,\n",
    "    splitting_seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655ebde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.train_dataset.data # This is Train_X\n",
    "Data.train_dataset.targets # This is Train_y\n",
    "\n",
    "print(f\"Number of classes: {Data.num_classes}\")\n",
    "print(f\"Array of targets: \\n {Data.test_dataset.targets}\")\n",
    "df = pd.DataFrame(\n",
    "    Data.train_dataset.data.numpy(),\n",
    "    # columns=[f'feature_{i}' \n",
    "    columns=[f'x_{i}' \n",
    "             for i in range(Data.train_dataset.data.shape[1])])\n",
    "\n",
    "# Add target column\n",
    "df['target'] = [i for i in Data.train_dataset.targets.numpy()]\n",
    "\n",
    "# Print 3 rows\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27509ea8",
   "metadata": {},
   "source": [
    "Next, we’ll simulate a **partial‐label learning** or **noisy-label** setting by corrupting each true label with **M**:\n",
    "\n",
    "1. **Instantiate** a `Weakener` with the number of true classes.  \n",
    "2. **Build** a mixing matrix via `generate_M(model_class='pll', corr_p=…)` \n",
    "3. **Generate** weak labels with `generate_weak`, which returns:\n",
    "   - `z`: the integer index of the weak‐label   \n",
    "   - `w`: a binary matrix of shape `(n_samples, n_classes)` indicating the candidate labels  \n",
    "4. **Insert** the partial labels into our Data using `include_weak(w)`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2587776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate transition matrix M\n",
    "corr_p = 0.2\n",
    "weakener = Weakener(true_classes=Data.num_classes)\n",
    "\n",
    "weakener.generate_M(model_class='pll', corr_p=0.2) # For partial label learning (PLL)\n",
    "#weakener.generate_M(model_class='unif_noise', corr_p=0.5) # For noisy labels\n",
    "\n",
    "print(f\"Generated M matrix:\\n\"\n",
    "      f\"{np.array2string(weakener.M, precision=4, suppress_small=True)}\")\n",
    "\n",
    "# Generate weak labels z\n",
    "true_onehot = Data.train_dataset.targets  # shape: (n_samples, n_classes)\n",
    "z = weakener.generate_weak(\n",
    "    true_onehot, compute_w=True, compute_Y=False, compute_Y_opt=False,\n",
    "    compute_Y_conv=False, compute_Y_opt_conv=False)\n",
    "print(f\"Generated weak labels z:\\n{z}\")\n",
    "\n",
    "# Compute virtual labels\n",
    "print(\"Computing virtual labels...\")\n",
    "# Since z[i] is an integer, row i must contain the z[i]-th row of weakener.Z\n",
    "virtual_labels = weakener.Z[z]\n",
    "\n",
    "# Add weak and virtual labels to the dataset\n",
    "Data.include_weak(z)\n",
    "Data.include_virtual(virtual_labels, initial_weight=1/Data.num_classes)\n",
    "\n",
    "# Get dataloaders with weak labels and indices\n",
    "train_loader, test_loader = Data.get_dataloader(\n",
    "    weak_labels='all', get_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786e4aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "xb, wb, vb, cb, yb, ib = batch\n",
    "print(f\"Inputs batch: shape {xb.shape}\")\n",
    "print(f\"Weak (partial) labels: shape {wb.shape}, first row: {wb[0]}\")\n",
    "print(f\"Virtual labels: shape {vb.shape}, first row: {vb[0]}\")\n",
    "print(f\"Weights: shape {cb.shape}, first row: {cb[0]}\")\n",
    "print(f\"True one-hot labels shape: {yb.shape}, first row: {yb[0]}\")\n",
    "print(f\"Indices batch: shape {ib.shape}, first row: {ib[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a69c4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_df = pd.DataFrame(Data.train_dataset.data.numpy(), columns=[f'x_{i}' for i in range(Data.train_dataset.data.shape[1])])\n",
    "df['target'] = [i for i in weakener.w.numpy()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a53b540",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **Instantiate** the model (e.g. `MLP`) with its input/output dimensions.   \n",
    "2. **Choose** the optimizer and set hyperparameters.  \n",
    "3. **Define** the loss function.\n",
    "\n",
    "We also could do a learning rate scheduler (e.g. `StepLR`) to decrease the LR over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f8ac2a",
   "metadata": {},
   "source": [
    "## Training the MLP (using `train_test_loop.py`)\n",
    "\n",
    "1. **Set** training hyperparameters  \n",
    "2. **Call** `train_and_evaluate(model, train_loader, test_loader, optimizer, pll_loss, num_epochs, corr_p)`\n",
    "3. **Plot** results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca58784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##########################\n",
    "# Define model and optimizer\n",
    "def get_model(Data, opt_alg='adam', lr=1e-6, momentum=0.9):\n",
    "    \"\"\" Define model and optimizer\n",
    "    Parameters\n",
    "        Data: dataset object with num_classes and num_features attributes\n",
    "    Returns\n",
    "        model: the neural network model\n",
    "        optimizer: the optimizer\n",
    "    \"\"\"\n",
    "\n",
    "    model = MLP(\n",
    "        input_size=Data.num_features,\n",
    "        hidden_sizes=[],\n",
    "        output_size=Data.num_classes,\n",
    "        dropout_p=0,\n",
    "        bn=False,\n",
    "        activation='relu')\n",
    "\n",
    "    if opt_alg == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    elif opt_alg == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {opt_alg}\") \n",
    "\n",
    "    return model, optimizer\n",
    "\n",
    "# ########################\n",
    "# Visualization of results\n",
    "def plot_results(results_df):\n",
    "    \"\"\" Plot training results\n",
    "    Parameters\n",
    "        results_df: DataFrame with training results\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up a wide figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    # Loss curves\n",
    "    ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Loss Curve')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Accuracy curves\n",
    "    ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "    ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Accuracy Curves')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fe46d0",
   "metadata": {},
   "source": [
    "Now we define a method with the steps of each simulation to test a learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d040511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(Data, loss, num_epochs=10, optimizer_name='adam', \n",
    "                   loss_code=None, pseudolabel_model=None, lr=1e-6, corr_p=0.2,\n",
    "                   phi=0.8):\n",
    "    \"\"\"Run a training simulation with the specified parameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Data : dataset object\n",
    "        The dataset object containing training and test data.\n",
    "    loss : loss function object\n",
    "        The loss function to use for training.\n",
    "    num_epochs : int, optional\n",
    "        The number of training epochs (default is 10).\n",
    "    optimizer_name : str, optional\n",
    "        The name of the optimizer to use (default is 'adam').\n",
    "    loss_code : str, optional\n",
    "        A code representing the loss function (for metadata) (default is None).\n",
    "    pseudolabel_model : str, optional\n",
    "        A string indicating the pseudolabel model used (for metadata)\n",
    "        (default is None).\n",
    "    lr : float, optional\n",
    "        The learning rate for the optimizer (default is 1e-6).\n",
    "    corr_p : float, optional\n",
    "        The corruption probability used in the weak label generation \n",
    "        (for metadata) (default is 0.2).\n",
    "    phi : float, optional\n",
    "        The phi parameter for the training loop (default is 0.8).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Get model and optimizer\n",
    "    model, optimizer = get_model(Data, opt_alg=optimizer_name, lr=lr)\n",
    "\n",
    "    # 2. Run the training + evaluation loop\n",
    "    model, results_df = train_and_evaluate(\n",
    "        model, train_loader, test_loader, optimizer, loss, num_epochs,\n",
    "        phi=phi, pseudolabel_model=pseudolabel_model)\n",
    "\n",
    "    # 3. Save simultation attributes in a metadata dictionary\n",
    "    metadata = {'pseudo_label_model': pseudolabel_model, 'loss_name': loss_code, \n",
    "                'corr_p': corr_p, 'optimizer': type(optimizer).__name__,\n",
    "                'initial_lr': optimizer.param_groups[0]['lr'], 'phi': phi}\n",
    "    print(pd.DataFrame([metadata]).T)\n",
    "\n",
    "    # 4. View the epoch‐by‐epoch results\n",
    "    plot_results(results_df)\n",
    "\n",
    "    return model, optimizer, metadata, results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4177e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####################################\n",
    "# Common parameters for all simulations\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 60\n",
    "optimizer_name = 'adam'\n",
    "lr = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ba4319",
   "metadata": {},
   "source": [
    "## Running experiments  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49911e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################\n",
    "# Testing IR Loss\n",
    "\n",
    "loss_code = \"cross_entropy\"\n",
    "loss = IRLoss(weakener.M, loss_code=loss_code)\n",
    "pseudolabel_model = 'IR'\n",
    "\n",
    "model, optimizer, metadata, results_df = run_simulation(\n",
    "    Data, loss, num_epochs=num_epochs, optimizer_name=optimizer_name, \n",
    "    loss_code=loss_code, pseudolabel_model=pseudolabel_model, lr=lr,\n",
    "    corr_p=corr_p)\n",
    "\n",
    "results_df.head(num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d836ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################\n",
    "# Testing PiCO Loss\n",
    "\n",
    "loss_code = \"cross_entropy\"\n",
    "loss = PiCOLoss(loss_code=loss_code)\n",
    "pseudolabel_model = 'PiCO'\n",
    "phi = 0.8\n",
    "\n",
    "model, optimizer, metadata, results_df = run_simulation(\n",
    "    Data, loss, num_epochs=num_epochs, optimizer_name=optimizer_name, \n",
    "    loss_code=loss_code, pseudolabel_model=pseudolabel_model, lr=lr,\n",
    "    corr_p=corr_p, phi=phi)\n",
    "\n",
    "results_df.head(num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b7ec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################\n",
    "# Marginal Chain\n",
    "\n",
    "loss_code = \"cross_entropy\"\n",
    "loss = MarginalChainProperLoss(weakener.M, loss_code=loss_code)\n",
    "pseudolabel_model = 'MC'\n",
    "\n",
    "model, optimizer, metadata, results_df = run_simulation(\n",
    "    Data, loss, num_epochs=num_epochs, optimizer_name=optimizer_name, \n",
    "    loss_code=loss_code, pseudolabel_model=pseudolabel_model, lr=lr,\n",
    "    corr_p=corr_p, phi=phi)\n",
    "\n",
    "results_df.head(num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89db1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################\n",
    "# Forward Proper Loss\n",
    "\n",
    "loss_code = \"cross_entropy\"\n",
    "loss = ForwardProperLoss(weakener.M, loss_code=loss_code)\n",
    "pseudolabel_model = 'FWD'\n",
    "\n",
    "model, optimizer, metadata, results_df = run_simulation(\n",
    "    Data, loss, num_epochs=num_epochs, optimizer_name=optimizer_name, \n",
    "    loss_code=loss_code, pseudolabel_model=pseudolabel_model, lr=lr,\n",
    "    corr_p=corr_p, phi=phi)\n",
    "\n",
    "results_df.head(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0822459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############\n",
    "# Majorization-Minimization (Uppder bound)\n",
    "\n",
    "loss_code = \"cross_entropy\"\n",
    "loss = UpperBoundWeakProperLoss(weakener.M, loss_code=loss_code)\n",
    "pseudolabel_model = 'MM'\n",
    "\n",
    "model, optimizer, metadata, results_df = run_simulation(\n",
    "    Data, loss, num_epochs=num_epochs, optimizer_name=optimizer_name, \n",
    "    loss_code=loss_code, pseudolabel_model=pseudolabel_model, lr=lr,\n",
    "    corr_p=corr_p, phi=phi)\n",
    "\n",
    "results_df.head(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8baa1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################\n",
    "# Testing IR Loss\n",
    "\n",
    "loss_code = \"ps_2\"\n",
    "loss = IRLoss(weakener.M, loss_code=loss_code)\n",
    "pseudolabel_model = 'IR'\n",
    "\n",
    "model, optimizer, metadata, results_df = run_simulation(\n",
    "    Data, loss, num_epochs=num_epochs, optimizer_name=optimizer_name, \n",
    "    loss_code=loss_code, pseudolabel_model=pseudolabel_model, lr=lr,\n",
    "    corr_p=corr_p)\n",
    "\n",
    "results_df.head(num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aa371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################\n",
    "# Testing PiCO Loss\n",
    "\n",
    "loss_code = \"ps_2\"\n",
    "loss = PiCOLoss(loss_code=loss_code)\n",
    "pseudolabel_model = 'PiCO'\n",
    "phi = 0.8\n",
    "\n",
    "model, optimizer, metadata, results_df = run_simulation(\n",
    "    Data, loss, num_epochs=num_epochs, optimizer_name=optimizer_name, \n",
    "    loss_code=loss_code, pseudolabel_model=pseudolabel_model, lr=lr,\n",
    "    corr_p=corr_p, phi=phi)\n",
    "\n",
    "results_df.head(num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d20a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################\n",
    "# Marginal Chain\n",
    "\n",
    "loss_code = \"ps_2\"\n",
    "loss = MarginalChainProperLoss(weakener.M, loss_code=loss_code)\n",
    "pseudolabel_model = 'MC'\n",
    "\n",
    "model, optimizer, metadata, results_df = run_simulation(\n",
    "    Data, loss, num_epochs=num_epochs, optimizer_name=optimizer_name, \n",
    "    loss_code=loss_code, pseudolabel_model=pseudolabel_model, lr=lr,\n",
    "    corr_p=corr_p, phi=phi)\n",
    "\n",
    "results_df.head(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31519ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_code = \"ps_2\"\n",
    "loss = ForwardProperLoss(weakener.M, loss_code=loss_code)\n",
    "pseudolabel_model = 'FWD'\n",
    "\n",
    "model, optimizer, metadata, results_df = run_simulation(\n",
    "    Data, loss, num_epochs=num_epochs, optimizer_name=optimizer_name, \n",
    "    loss_code=loss_code, pseudolabel_model=pseudolabel_model, lr=lr,\n",
    "    corr_p=corr_p, phi=phi)\n",
    "\n",
    "results_df.head(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d1d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_code = \"ps_2\"\n",
    "loss = UpperBoundWeakProperLoss(weakener.M, loss_code=loss_code)\n",
    "pseudolabel_model = 'MM'\n",
    "\n",
    "\n",
    "model, optimizer, metadata, results_df = run_simulation(\n",
    "    Data, loss, num_epochs=num_epochs, optimizer_name=optimizer_name, \n",
    "    loss_code=loss_code, pseudolabel_model=pseudolabel_model, lr=lr,\n",
    "    corr_p=corr_p, phi=phi)\n",
    "\n",
    "results_df.head(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567014a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################\n",
    "# Testing IR Loss\n",
    "\n",
    "loss_code = \"spherical\"\n",
    "loss = IRLoss(weakener.M, loss_code=loss_code)\n",
    "pseudolabel_model = 'IR'\n",
    "\n",
    "model, optimizer, metadata, results_df = run_simulation(\n",
    "    Data, loss, num_epochs=num_epochs, optimizer_name=optimizer_name, \n",
    "    loss_code=loss_code, pseudolabel_model=pseudolabel_model, lr=lr,\n",
    "    corr_p=corr_p)\n",
    "\n",
    "results_df.head(num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f025bba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################\n",
    "# Testing PiCO Loss\n",
    "\n",
    "loss_code = \"spherical\"\n",
    "loss = PiCOLoss(loss_code=loss_code)\n",
    "pseudolabel_model = 'PiCO'\n",
    "phi = 0.8\n",
    "\n",
    "model, optimizer, metadata, results_df = run_simulation(\n",
    "    Data, loss, num_epochs=num_epochs, optimizer_name=optimizer_name, \n",
    "    loss_code=loss_code, pseudolabel_model=pseudolabel_model, lr=lr,\n",
    "    corr_p=corr_p, phi=phi)\n",
    "\n",
    "results_df.head(num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f687be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################\n",
    "# Marginal Chain\n",
    "\n",
    "loss_code = \"spherical\"\n",
    "loss = MarginalChainProperLoss(weakener.M, loss_code=loss_code)\n",
    "pseudolabel_model = 'MC'\n",
    "\n",
    "model, optimizer, metadata, results_df = run_simulation(\n",
    "    Data, loss, num_epochs=num_epochs, optimizer_name=optimizer_name, \n",
    "    loss_code=loss_code, pseudolabel_model=pseudolabel_model, lr=lr,\n",
    "    corr_p=corr_p, phi=phi)\n",
    "\n",
    "results_df.head(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf0aae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_code = \"spherical\"\n",
    "loss = ForwardProperLoss(weakener.M, loss_code=loss_code)\n",
    "pseudolabel_model = 'FWD'\n",
    "\n",
    "model, optimizer, metadata, results_df = run_simulation(\n",
    "    Data, loss, num_epochs=num_epochs, optimizer_name=optimizer_name, \n",
    "    loss_code=loss_code, pseudolabel_model=pseudolabel_model, lr=lr,\n",
    "    corr_p=corr_p, phi=phi)\n",
    "\n",
    "results_df.head(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df81cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_code = \"spherical\"\n",
    "loss = UpperBoundWeakProperLoss(weakener.M, loss_code=loss_code)\n",
    "pseudolabel_model = 'MM'\n",
    "\n",
    "\n",
    "model, optimizer, metadata, results_df = run_simulation(\n",
    "    Data, loss, num_epochs=num_epochs, optimizer_name=optimizer_name, \n",
    "    loss_code=loss_code, pseudolabel_model=pseudolabel_model, lr=lr,\n",
    "    corr_p=corr_p, phi=phi)\n",
    "\n",
    "results_df.head(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df728649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################\n",
    "# Testing IR Loss\n",
    "\n",
    "loss_code = \"brier\"\n",
    "loss = IRLoss(weakener.M, loss_code=loss_code)\n",
    "pseudolabel_model = 'IR'\n",
    "\n",
    "model, optimizer, metadata, results_df = run_simulation(\n",
    "    Data, loss, num_epochs=num_epochs, optimizer_name=optimizer_name, \n",
    "    loss_code=loss_code, pseudolabel_model=pseudolabel_model, lr=lr,\n",
    "    corr_p=corr_p)\n",
    "\n",
    "results_df.head(num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eb4298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################\n",
    "# Testing PiCO Loss\n",
    "\n",
    "loss_code = \"brier\"\n",
    "loss = PiCOLoss(loss_code=loss_code)\n",
    "pseudolabel_model = 'PiCO'\n",
    "phi = 0.8\n",
    "\n",
    "model, optimizer, metadata, results_df = run_simulation(\n",
    "    Data, loss, num_epochs=num_epochs, optimizer_name=optimizer_name, \n",
    "    loss_code=loss_code, pseudolabel_model=pseudolabel_model, lr=lr,\n",
    "    corr_p=corr_p, phi=phi)\n",
    "\n",
    "results_df.head(num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13076ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##############\n",
    "# Marginal Chain\n",
    "\n",
    "loss_code = \"brier\"\n",
    "loss = MarginalChainProperLoss(weakener.M, loss_code=loss_code)\n",
    "pseudolabel_model = 'MC'\n",
    "\n",
    "model, optimizer, metadata, results_df = run_simulation(\n",
    "    Data, loss, num_epochs=num_epochs, optimizer_name=optimizer_name, \n",
    "    loss_code=loss_code, pseudolabel_model=pseudolabel_model, lr=lr,\n",
    "    corr_p=corr_p, phi=phi)\n",
    "\n",
    "results_df.head(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bedf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_code = \"brier\"\n",
    "loss = ForwardProperLoss(weakener.M, loss_code=loss_code)\n",
    "pseudolabel_model = 'FWD'\n",
    "\n",
    "model, optimizer, metadata, results_df = run_simulation(\n",
    "    Data, loss, num_epochs=num_epochs, optimizer_name=optimizer_name, \n",
    "    loss_code=loss_code, pseudolabel_model=pseudolabel_model, lr=lr,\n",
    "    corr_p=corr_p, phi=phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48231835",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_code = \"brier\"\n",
    "loss = UpperBoundWeakProperLoss(weakener.M, loss_code=loss_code)\n",
    "pseudolabel_model = 'MM'\n",
    "\n",
    "\n",
    "model, optimizer, metadata, results_df = run_simulation(\n",
    "    Data, loss, num_epochs=num_epochs, optimizer_name=optimizer_name, \n",
    "    loss_code=loss_code, pseudolabel_model=pseudolabel_model, lr=lr,\n",
    "    corr_p=corr_p, phi=phi)\n",
    "\n",
    "results_df.head(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e131f1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################\n",
    "# Testing IR Loss\n",
    "\n",
    "loss_code = \"tsallis_0.2\"\n",
    "loss = IRLoss(weakener.M, loss_code=loss_code)\n",
    "pseudolabel_model = 'IR'\n",
    "\n",
    "model, optimizer, metadata, results_df = run_simulation(\n",
    "    Data, loss, num_epochs=num_epochs, optimizer_name=optimizer_name, \n",
    "    loss_code=loss_code, pseudolabel_model=pseudolabel_model, lr=lr,\n",
    "    corr_p=corr_p)\n",
    "\n",
    "results_df.head(num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fdb7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################\n",
    "# Testing PiCO Loss\n",
    "\n",
    "loss_code = \"tsallis_0.2\"\n",
    "loss = PiCOLoss(loss_code=loss_code)\n",
    "pseudolabel_model = 'PiCO'\n",
    "phi = 0.8\n",
    "\n",
    "model, optimizer, metadata, results_df = run_simulation(\n",
    "    Data, loss, num_epochs=num_epochs, optimizer_name=optimizer_name, \n",
    "    loss_code=loss_code, pseudolabel_model=pseudolabel_model, lr=lr,\n",
    "    corr_p=corr_p, phi=phi)\n",
    "\n",
    "results_df.head(num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34351956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##############\n",
    "# Marginal Chain\n",
    "\n",
    "loss_code = \"tsallis_0.2\"\n",
    "loss = MarginalChainProperLoss(weakener.M, loss_code=loss_code)\n",
    "pseudolabel_model = 'MC'\n",
    "\n",
    "model, optimizer, metadata, results_df = run_simulation(\n",
    "    Data, loss, num_epochs=num_epochs, optimizer_name=optimizer_name, \n",
    "    loss_code=loss_code, pseudolabel_model=pseudolabel_model, lr=lr,\n",
    "    corr_p=corr_p, phi=phi)\n",
    "\n",
    "results_df.head(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f39e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_code = \"tsallis_0.2\"\n",
    "loss = ForwardProperLoss(weakener.M, loss_code=loss_code)\n",
    "pseudolabel_model = 'FWD'\n",
    "\n",
    "model, optimizer, metadata, results_df = run_simulation(\n",
    "    Data, loss, num_epochs=num_epochs, optimizer_name=optimizer_name, \n",
    "    loss_code=loss_code, pseudolabel_model=pseudolabel_model, lr=lr,\n",
    "    corr_p=corr_p, phi=phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3583bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_code = \"tsallis_0.2\"\n",
    "loss = UpperBoundWeakProperLoss(weakener.M, loss_code=loss_code)\n",
    "pseudolabel_model = 'MM'\n",
    "\n",
    "model, optimizer, metadata, results_df = run_simulation(\n",
    "    Data, loss, num_epochs=num_epochs, optimizer_name=optimizer_name, \n",
    "    loss_code=loss_code, pseudolabel_model=pseudolabel_model, lr=lr,\n",
    "    corr_p=corr_p, phi=phi)\n",
    "\n",
    "results_df.head(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ea7328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% MC vs FWD comparison (per base loss): save CSV + GridSpec figure\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_seed_all(seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    return str(s).replace(\"/\", \"_\").replace(\" \", \"_\").replace(\".\", \"_\")\n",
    "\n",
    "# 你要对比的 base losses（自己按需增减）\n",
    "base_losses = [\n",
    "    \"cross_entropy\",\n",
    "    \"brier\",\n",
    "    \"ps_2\",\n",
    "    \"tsallis_0.2\",\n",
    "]\n",
    "base_losses = [\n",
    "    \"cross_entropy\",\n",
    "]\n",
    "\n",
    "# 输出目录\n",
    "out_dir = Path(\"outputs/mc_vs_fwd\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 是否在 accuracy 图里加 train_acc（虚线）\n",
    "show_train_acc = True\n",
    "\n",
    "# 为了公平：MC / FWD 每次都用同一个初始化 seed\n",
    "run_seed = 123\n",
    "\n",
    "for loss_code in base_losses:\n",
    "    print(f\"\\n=== Running base loss: {loss_code} ===\")\n",
    "\n",
    "    # ---------- MC ----------\n",
    "    set_seed_all(run_seed)\n",
    "    model_mc, opt_mc = get_model(Data, opt_alg=optimizer_name, lr=lr)\n",
    "    loss_mc = MarginalChainProperLoss(weakener.M, loss_code=loss_code)\n",
    "\n",
    "    _, df_mc = train_and_evaluate(\n",
    "        model_mc, train_loader, test_loader, opt_mc, loss_mc,\n",
    "        num_epochs=2,\n",
    "        pseudolabel_model=\"MC\",\n",
    "        phi=phi,\n",
    "        seed=run_seed,\n",
    "        sound=10,   # 想少打印就调大\n",
    "    )\n",
    "\n",
    "    # ---------- FWD ----------\n",
    "    set_seed_all(run_seed)\n",
    "    model_fwd, opt_fwd = get_model(Data, opt_alg=optimizer_name, lr=lr)\n",
    "    loss_fwd = ForwardProperLoss(weakener.M, loss_code=loss_code)\n",
    "\n",
    "    _, df_fwd = train_and_evaluate(\n",
    "        model_fwd, train_loader, test_loader, opt_fwd, loss_fwd,\n",
    "        num_epochs=2,\n",
    "        pseudolabel_model=\"FWD\",\n",
    "        phi=phi,\n",
    "        seed=run_seed,\n",
    "        sound=10,\n",
    "    )\n",
    "\n",
    "    tag = sanitize(loss_code)\n",
    "\n",
    "    # 存 CSV（分别存 MC / FWD）\n",
    "    df_mc.to_csv(out_dir / f\"{tag}_MC.csv\", index=False)\n",
    "    df_fwd.to_csv(out_dir / f\"{tag}_FWD.csv\", index=False)\n",
    "\n",
    "    # ---------- 画你要的 GridSpec 图 ----------\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    gs = GridSpec(\n",
    "        2, 2, figure=fig,\n",
    "        width_ratios=[2.2, 1.2],\n",
    "        wspace=0.35, hspace=0.40\n",
    "    )\n",
    "\n",
    "    ax_acc = fig.add_subplot(gs[:, 0])   # 左列跨两行\n",
    "    ax_mc  = fig.add_subplot(gs[0, 1])   # 右上：MC loss\n",
    "    ax_fwd = fig.add_subplot(gs[1, 1])   # 右下：FWD loss\n",
    "\n",
    "    # 左：Accuracy（默认画 test_acc）\n",
    "    ax_acc.plot(df_mc[\"epoch\"],  df_mc[\"test_acc\"],  label=\"MC test_acc\")\n",
    "    ax_acc.plot(df_fwd[\"epoch\"], df_fwd[\"test_acc\"], label=\"FWD test_acc\")\n",
    "\n",
    "    if show_train_acc and (\"train_acc\" in df_mc.columns) and (\"train_acc\" in df_fwd.columns):\n",
    "        ax_acc.plot(df_mc[\"epoch\"],  df_mc[\"train_acc\"],  linestyle=\"--\", label=\"MC train_acc\")\n",
    "        ax_acc.plot(df_fwd[\"epoch\"], df_fwd[\"train_acc\"], linestyle=\"--\", label=\"FWD train_acc\")\n",
    "\n",
    "    ax_acc.set_title(f\"Accuracy — base loss: {loss_code}\")\n",
    "    ax_acc.set_xlabel(\"Epoch\")\n",
    "    ax_acc.set_ylabel(\"Accuracy\")\n",
    "    ax_acc.grid(True, alpha=0.3)\n",
    "    ax_acc.legend()\n",
    "\n",
    "    # 右上：MC train loss\n",
    "    ax_mc.plot(df_mc[\"epoch\"], df_mc[\"train_loss\"])\n",
    "    ax_mc.set_title(\"MC train_loss\")\n",
    "    ax_mc.set_xlabel(\"Epoch\")\n",
    "    ax_mc.set_ylabel(\"Loss\")\n",
    "    ax_mc.grid(True, alpha=0.3)\n",
    "\n",
    "    # 右下：FWD train loss\n",
    "    ax_fwd.plot(df_fwd[\"epoch\"], df_fwd[\"train_loss\"])\n",
    "    ax_fwd.set_title(\"FWD train_loss\")\n",
    "    ax_fwd.set_xlabel(\"Epoch\")\n",
    "    ax_fwd.set_ylabel(\"Loss\")\n",
    "    ax_fwd.grid(True, alpha=0.3)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # 存图：PNG + PDF\n",
    "    fig.savefig(out_dir / f\"{tag}_MC_vs_FWD.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    fig.savefig(out_dir / f\"{tag}_MC_vs_FWD.pdf\", bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "print(f\"\\nAll done. Files saved under: {out_dir.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd1f219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WeakMC2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
