{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28621a1e",
   "metadata": {},
   "source": [
    "# Example of the usage of the Weak label classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d4dcfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f29cb012",
   "metadata": {},
   "source": [
    "We first need to load:\n",
    "\n",
    "1. **Standard Python libraries** for data handling and reproducibility.  \n",
    "2. **PyTorch** (and its submodules) for model definition, training, and data loading.  \n",
    "3. **Custom modules** from this project:\n",
    "   - **`train_test_loop`**: provides the `train_and_evaluate` function to run training and evaluation loops.  \n",
    "   - **`losses`**: contains various weak‐label‐aware loss functions like `FwdBwdLoss`.  \n",
    "   - **`weakener`**: implements the `Weakener` class for generating noisy/weak labels.  \n",
    "   - **`model`**: defines model architectures .\n",
    "   - **`dataset`**: provides `Data_handling` (and other dataset classes) for loading and splitting data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f62fdf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# PyTorch core\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Custom project modules\n",
    "from utils.train_test_loop import train_and_evaluate\n",
    "from utils.losses import FwdLoss, EMLoss, FwdBwdLoss, MarginalChainLoss\n",
    "from utils.losses1 import MarginalChainProperLoss, ForwardProperLoss, scoring_matrix\n",
    "from utils.losses1 import UpperBoundWeakProperLoss\n",
    "from utils.dataset_visualization import visualize_dataset\n",
    "from src.weakener import Weakener\n",
    "from src.model import MLP\n",
    "from src.dataset import Data_handling\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68db405b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73f12c25",
   "metadata": {},
   "source": [
    "## Loading and Visualizing Iris\n",
    "\n",
    "1. **Instantiate** our `Data_handling` class to load the Iris dataset from OpenML (ID 61) using an 80/20 train/test split.  \n",
    "2. **Retrieve** the raw arrays of features and labels via `get_data()`.  \n",
    "3. **Combine** the train and test portions back into a single DataFrame \n",
    "4. **Visualize** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cb6a1f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e44cfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'mnist'\n",
    "Data = Data_handling(\n",
    "    # dataset='mnist',\n",
    "    dataset=dataset_name,\n",
    "    train_size=0.8,\n",
    "    test_size=0.2,\n",
    "    batch_size=64,\n",
    "    shuffling=True,\n",
    "    splitting_seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "655ebde9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Data.train_dataset.data # This is Train_X\\nData.train_dataset.targets # This is Train_y\\nprint(Data.test_dataset.targets)\\ndf = pd.DataFrame(Data.train_dataset.data.numpy(), columns=[f'feature_{i}' for i in range(Data.train_dataset.data.shape[1])])\\ndf['target'] = [i for i in Data.train_dataset.targets.numpy()]\\ndf \""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Data.train_dataset.data # This is Train_X\n",
    "Data.train_dataset.targets # This is Train_y\n",
    "print(Data.test_dataset.targets)\n",
    "df = pd.DataFrame(Data.train_dataset.data.numpy(), columns=[f'feature_{i}' for i in range(Data.train_dataset.data.shape[1])])\n",
    "df['target'] = [i for i in Data.train_dataset.targets.numpy()]\n",
    "df \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b001a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e126947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" df_2_plot = df.iloc[0:1000]\\nfeatures = ['feature_102', 'feature_103']\\nvisualize_dataset(\\n    df_2_plot,\\n    features=features,\\n    classes=Data.num_classes,\\n    title=dataset_name,\\n) \""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" df_2_plot = df.iloc[0:1000]\n",
    "features = ['feature_102', 'feature_103']\n",
    "visualize_dataset(\n",
    "    df_2_plot,\n",
    "    features=features,\n",
    "    classes=Data.num_classes,\n",
    "    title=dataset_name,\n",
    ") \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68ce7928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' df_2_plot[[features[0], features[1]]] '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" df_2_plot[[features[0], features[1]]] \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27509ea8",
   "metadata": {},
   "source": [
    "Next, we’ll simulate a **partial‐label learning** or **noisy-label** setting by corrupting each true label with **M**:\n",
    "\n",
    "1. **Instantiate** a `Weakener` with the number of true classes.  \n",
    "2. **Build** a mixing matrix via `generate_M(model_class='pll', corr_p=…)` \n",
    "3. **Generate** weak labels with `generate_weak`, which returns:\n",
    "   - `z`: the integer index of the weak‐label   \n",
    "   - `w`: a binary matrix of shape `(n_samples, n_classes)` indicating the candidate labels  \n",
    "4. **Insert** the partial labels into our Data using `include_weak(w)`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2587776a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated M matrix:\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Generated z (noisy labels):\n",
      "tensor([4, 9, 5,  ..., 4, 3, 1], dtype=torch.int32)\n",
      "Inputs batch shape: torch.Size([64, 784])\n",
      "Weak (partial) labels shape: torch.Size([64])\n",
      "True one-hot labels shape: torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "corr_p = 0.0\n",
    "weakener = Weakener(true_classes=Data.num_classes)\n",
    "weakener.generate_M(model_class='pll', corr_p=corr_p)\n",
    "# weakener.generate_M(model_class='unif_noise', corr_p=0.5) #Try this for noisy labels\n",
    "print(f\"Generated M matrix:\\n{weakener.M}\")\n",
    "true_onehot = Data.train_dataset.targets  # shape: (n_samples, n_classes)\n",
    "\n",
    "z = weakener.generate_weak(true_onehot)\n",
    "print(f\"Generated z (noisy labels):\\n{z}\")\n",
    "#print(f\"Generated w (multi-label matrix):\\n{w}\")\n",
    "\n",
    "Data.include_weak(z)\n",
    "\n",
    "train_loader, test_loader = Data.get_dataloader(weak_labels='weak')\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "xb, wb, yb = batch\n",
    "print(f\"Inputs batch shape: {xb.shape}\")\n",
    "print(f\"Weak (partial) labels shape: {wb.shape}\")\n",
    "print(f\"True one-hot labels shape: {yb.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a69c4f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>weak_0</th>\n",
       "      <th>weak_1</th>\n",
       "      <th>weak_2</th>\n",
       "      <th>weak_3</th>\n",
       "      <th>weak_4</th>\n",
       "      <th>weak_5</th>\n",
       "      <th>weak_6</th>\n",
       "      <th>weak_7</th>\n",
       "      <th>weak_8</th>\n",
       "      <th>weak_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 795 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "1            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "2            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "3            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "4            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "59995        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "59996        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "59997        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "59998        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "59999        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "\n",
       "       feature_6  feature_7  feature_8  feature_9  ...  weak_0  weak_1  \\\n",
       "0            0.0        0.0        0.0        0.0  ...     0.0     0.0   \n",
       "1            0.0        0.0        0.0        0.0  ...     1.0     0.0   \n",
       "2            0.0        0.0        0.0        0.0  ...     0.0     0.0   \n",
       "3            0.0        0.0        0.0        0.0  ...     0.0     1.0   \n",
       "4            0.0        0.0        0.0        0.0  ...     0.0     0.0   \n",
       "...          ...        ...        ...        ...  ...     ...     ...   \n",
       "59995        0.0        0.0        0.0        0.0  ...     0.0     0.0   \n",
       "59996        0.0        0.0        0.0        0.0  ...     0.0     0.0   \n",
       "59997        0.0        0.0        0.0        0.0  ...     0.0     0.0   \n",
       "59998        0.0        0.0        0.0        0.0  ...     0.0     0.0   \n",
       "59999        0.0        0.0        0.0        0.0  ...     0.0     0.0   \n",
       "\n",
       "       weak_2  weak_3  weak_4  weak_5  weak_6  weak_7  weak_8  weak_9  \n",
       "0         0.0     0.0     0.0     1.0     0.0     0.0     0.0     0.0  \n",
       "1         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0  \n",
       "2         0.0     0.0     1.0     0.0     0.0     0.0     0.0     0.0  \n",
       "3         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0  \n",
       "4         0.0     0.0     0.0     0.0     0.0     0.0     0.0     1.0  \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...  \n",
       "59995     0.0     0.0     0.0     0.0     0.0     0.0     1.0     0.0  \n",
       "59996     0.0     1.0     0.0     0.0     0.0     0.0     0.0     0.0  \n",
       "59997     0.0     0.0     0.0     1.0     0.0     0.0     0.0     0.0  \n",
       "59998     0.0     0.0     0.0     0.0     1.0     0.0     0.0     0.0  \n",
       "59999     0.0     0.0     0.0     0.0     0.0     0.0     1.0     0.0  \n",
       "\n",
       "[60000 rows x 795 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#weak_df = pd.DataFrame(Data.train_dataset.data.numpy(), columns=[f'feature_{i}' for i in range(Data.train_dataset.data.shape[1])])\n",
    "#df['target'] = [i for i in weakener.w.numpy()]\n",
    "#df\n",
    "\n",
    "# 1) 展平成 (N, 3072)\n",
    "X = Data.train_dataset.data                # (N, 3, 32, 32)  (torch tensor)\n",
    "X2 = X.view(X.shape[0], -1).cpu().numpy()  # (N, 3072)\n",
    "\n",
    "weak_df = pd.DataFrame(X2, columns=[f'feature_{i}' for i in range(X2.shape[1])])\n",
    "\n",
    "# 2) 加 true label（如果 targets 是 one-hot，就转成 class index）\n",
    "y = Data.train_dataset.targets\n",
    "if hasattr(y, \"ndim\") and y.ndim == 2:\n",
    "    y = y.argmax(dim=1)\n",
    "weak_df[\"target\"] = y.cpu().numpy()\n",
    "\n",
    "# 3) 加 weak label（weakener.w 可能是一维或二维：做个兼容）\n",
    "w = weakener.w\n",
    "w_np = w.detach().cpu().numpy()\n",
    "\n",
    "if w_np.ndim == 1:\n",
    "    weak_df[\"weak\"] = w_np\n",
    "else:\n",
    "    # 如果是 one-hot / multi-hot (N,C)，你可以：\n",
    "    # A) 每一类一列（适合做统计）\n",
    "    for c in range(w_np.shape[1]):\n",
    "        weak_df[f\"weak_{c}\"] = w_np[:, c]\n",
    "    # 或 B) 压缩成“候选集合”（适合阅读）\n",
    "    # weak_df[\"weak_set\"] = [np.flatnonzero(row).tolist() for row in w_np]\n",
    "\n",
    "weak_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7430f462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_dataset(\n",
    "#     df,\n",
    "#     features=['feature_0', 'feature_1'],\n",
    "#     classes=3,\n",
    "#     title='Iris Samples with Pie Markers for Multi-Label Entries'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a53b540",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **Instantiate** the model (e.g. `MLP`) with its input/output dimensions.   \n",
    "2. **Choose** the optimizer and set hyperparameters.  \n",
    "3. **Define** the loss function.\n",
    "\n",
    "We also could do a learning rate scheduler (e.g. `StepLR`) to decrease the LR over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f8ac2a",
   "metadata": {},
   "source": [
    "## Training the MLP (using `train_test_loop.py`)\n",
    "\n",
    "1. **Set** training hyperparameters  \n",
    "2. **Call** `train_and_evaluate(model, train_loader, test_loader, optimizer, pll_loss, num_epochs, corr_p)`\n",
    "3. **Plot** results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fac0280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Initializing MLP model...\n",
      "784\n",
      "[256, 128]\n",
      "10\n",
      "Epoch 10/600: Train Loss: 0.0443, Train Acc: 0.0986, Test Acc: 0.1010, Train Detached Loss: 0.0449, Test Detached Loss: 0.0454, Learning Rate: 0.000001, Epoch Time: 1.53 seconds\n",
      "Epoch 20/600: Train Loss: 0.0434, Train Acc: 0.0994, Test Acc: 0.1023, Train Detached Loss: 0.0413, Test Detached Loss: 0.0415, Learning Rate: 0.000001, Epoch Time: 1.53 seconds\n",
      "Epoch 30/600: Train Loss: 0.0430, Train Acc: 0.1008, Test Acc: 0.0909, Train Detached Loss: 0.0401, Test Detached Loss: 0.0404, Learning Rate: 0.000001, Epoch Time: 1.89 seconds\n",
      "Epoch 40/600: Train Loss: 0.0417, Train Acc: 0.1006, Test Acc: 0.1032, Train Detached Loss: 0.0381, Test Detached Loss: 0.0382, Learning Rate: 0.000001, Epoch Time: 1.54 seconds\n",
      "Epoch 50/600: Train Loss: 0.0410, Train Acc: 0.0974, Test Acc: 0.0974, Train Detached Loss: 0.0404, Test Detached Loss: 0.0405, Learning Rate: 0.000001, Epoch Time: 1.56 seconds\n",
      "Epoch 60/600: Train Loss: 0.0402, Train Acc: 0.0987, Test Acc: 0.0971, Train Detached Loss: 0.0385, Test Detached Loss: 0.0387, Learning Rate: 0.000001, Epoch Time: 1.52 seconds\n",
      "Epoch 70/600: Train Loss: 0.0393, Train Acc: 0.1022, Test Acc: 0.0980, Train Detached Loss: 0.0395, Test Detached Loss: 0.0399, Learning Rate: 0.000001, Epoch Time: 1.83 seconds\n",
      "Epoch 80/600: Train Loss: 0.0389, Train Acc: 0.0999, Test Acc: 0.1009, Train Detached Loss: 0.0399, Test Detached Loss: 0.0401, Learning Rate: 0.000001, Epoch Time: 1.54 seconds\n",
      "Epoch 90/600: Train Loss: 0.0382, Train Acc: 0.0977, Test Acc: 0.1032, Train Detached Loss: 0.0394, Test Detached Loss: 0.0396, Learning Rate: 0.000001, Epoch Time: 1.59 seconds\n",
      "Epoch 100/600: Train Loss: 0.0374, Train Acc: 0.0984, Test Acc: 0.1012, Train Detached Loss: 0.0380, Test Detached Loss: 0.0383, Learning Rate: 0.000001, Epoch Time: 1.70 seconds\n",
      "Epoch 110/600: Train Loss: 0.0368, Train Acc: 0.1009, Test Acc: 0.0892, Train Detached Loss: 0.0362, Test Detached Loss: 0.0364, Learning Rate: 0.000001, Epoch Time: 1.58 seconds\n",
      "Epoch 120/600: Train Loss: 0.0357, Train Acc: 0.1020, Test Acc: 0.1032, Train Detached Loss: 0.0353, Test Detached Loss: 0.0354, Learning Rate: 0.000001, Epoch Time: 1.56 seconds\n",
      "Epoch 130/600: Train Loss: 0.0341, Train Acc: 0.2060, Test Acc: 0.2056, Train Detached Loss: 0.0339, Test Detached Loss: 0.0338, Learning Rate: 0.000001, Epoch Time: 1.88 seconds\n",
      "Epoch 140/600: Train Loss: 0.0334, Train Acc: 0.2094, Test Acc: 0.2166, Train Detached Loss: 0.0333, Test Detached Loss: 0.0333, Learning Rate: 0.000001, Epoch Time: 1.83 seconds\n",
      "Epoch 150/600: Train Loss: 0.0327, Train Acc: 0.2124, Test Acc: 0.2098, Train Detached Loss: 0.0326, Test Detached Loss: 0.0326, Learning Rate: 0.000001, Epoch Time: 1.61 seconds\n",
      "Epoch 160/600: Train Loss: 0.0322, Train Acc: 0.2087, Test Acc: 0.2209, Train Detached Loss: 0.0322, Test Detached Loss: 0.0322, Learning Rate: 0.000001, Epoch Time: 1.69 seconds\n",
      "Epoch 170/600: Train Loss: 0.0318, Train Acc: 0.2128, Test Acc: 0.2099, Train Detached Loss: 0.0316, Test Detached Loss: 0.0317, Learning Rate: 0.000001, Epoch Time: 1.51 seconds\n",
      "Epoch 180/600: Train Loss: 0.0312, Train Acc: 0.2434, Test Acc: 0.2570, Train Detached Loss: 0.0310, Test Detached Loss: 0.0311, Learning Rate: 0.000001, Epoch Time: 1.51 seconds\n",
      "Epoch 190/600: Train Loss: 0.0292, Train Acc: 0.2871, Test Acc: 0.3148, Train Detached Loss: 0.0283, Test Detached Loss: 0.0283, Learning Rate: 0.000001, Epoch Time: 1.51 seconds\n",
      "Epoch 200/600: Train Loss: 0.0277, Train Acc: 0.3202, Test Acc: 0.3506, Train Detached Loss: 0.0268, Test Detached Loss: 0.0268, Learning Rate: 0.000001, Epoch Time: 1.52 seconds\n",
      "Epoch 210/600: Train Loss: 0.0266, Train Acc: 0.3587, Test Acc: 0.4033, Train Detached Loss: 0.0256, Test Detached Loss: 0.0257, Learning Rate: 0.000001, Epoch Time: 1.52 seconds\n",
      "Epoch 220/600: Train Loss: 0.0251, Train Acc: 0.4195, Test Acc: 0.4450, Train Detached Loss: 0.0239, Test Detached Loss: 0.0240, Learning Rate: 0.000001, Epoch Time: 1.50 seconds\n",
      "Epoch 230/600: Train Loss: 0.0218, Train Acc: 0.5135, Test Acc: 0.5468, Train Detached Loss: 0.0203, Test Detached Loss: 0.0203, Learning Rate: 0.000001, Epoch Time: 1.51 seconds\n",
      "Epoch 240/600: Train Loss: 0.0192, Train Acc: 0.5395, Test Acc: 0.5810, Train Detached Loss: 0.0175, Test Detached Loss: 0.0174, Learning Rate: 0.000001, Epoch Time: 1.49 seconds\n",
      "Epoch 250/600: Train Loss: 0.0178, Train Acc: 0.5662, Test Acc: 0.6216, Train Detached Loss: 0.0163, Test Detached Loss: 0.0161, Learning Rate: 0.000001, Epoch Time: 1.63 seconds\n",
      "Epoch 260/600: Train Loss: 0.0169, Train Acc: 0.5933, Test Acc: 0.6385, Train Detached Loss: 0.0155, Test Detached Loss: 0.0154, Learning Rate: 0.000001, Epoch Time: 1.58 seconds\n",
      "Epoch 270/600: Train Loss: 0.0162, Train Acc: 0.6194, Test Acc: 0.6551, Train Detached Loss: 0.0149, Test Detached Loss: 0.0148, Learning Rate: 0.000001, Epoch Time: 1.50 seconds\n",
      "Epoch 280/600: Train Loss: 0.0154, Train Acc: 0.6575, Test Acc: 0.6907, Train Detached Loss: 0.0140, Test Detached Loss: 0.0140, Learning Rate: 0.000001, Epoch Time: 1.52 seconds\n",
      "Epoch 290/600: Train Loss: 0.0145, Train Acc: 0.7058, Test Acc: 0.7527, Train Detached Loss: 0.0129, Test Detached Loss: 0.0130, Learning Rate: 0.000001, Epoch Time: 1.51 seconds\n",
      "Epoch 300/600: Train Loss: 0.0133, Train Acc: 0.7441, Test Acc: 0.7923, Train Detached Loss: 0.0118, Test Detached Loss: 0.0119, Learning Rate: 0.000001, Epoch Time: 1.51 seconds\n",
      "Epoch 310/600: Train Loss: 0.0124, Train Acc: 0.7664, Test Acc: 0.8159, Train Detached Loss: 0.0109, Test Detached Loss: 0.0110, Learning Rate: 0.000001, Epoch Time: 1.50 seconds\n",
      "Epoch 320/600: Train Loss: 0.0115, Train Acc: 0.7895, Test Acc: 0.8341, Train Detached Loss: 0.0101, Test Detached Loss: 0.0102, Learning Rate: 0.000001, Epoch Time: 1.49 seconds\n",
      "Epoch 330/600: Train Loss: 0.0108, Train Acc: 0.8042, Test Acc: 0.8464, Train Detached Loss: 0.0095, Test Detached Loss: 0.0095, Learning Rate: 0.000001, Epoch Time: 1.52 seconds\n",
      "Epoch 340/600: Train Loss: 0.0101, Train Acc: 0.8196, Test Acc: 0.8572, Train Detached Loss: 0.0089, Test Detached Loss: 0.0089, Learning Rate: 0.000001, Epoch Time: 1.50 seconds\n",
      "Epoch 350/600: Train Loss: 0.0096, Train Acc: 0.8321, Test Acc: 0.8677, Train Detached Loss: 0.0084, Test Detached Loss: 0.0084, Learning Rate: 0.000001, Epoch Time: 1.50 seconds\n",
      "Epoch 360/600: Train Loss: 0.0091, Train Acc: 0.8436, Test Acc: 0.8756, Train Detached Loss: 0.0079, Test Detached Loss: 0.0080, Learning Rate: 0.000001, Epoch Time: 1.52 seconds\n",
      "Epoch 370/600: Train Loss: 0.0086, Train Acc: 0.8520, Test Acc: 0.8829, Train Detached Loss: 0.0075, Test Detached Loss: 0.0075, Learning Rate: 0.000001, Epoch Time: 1.50 seconds\n",
      "Epoch 380/600: Train Loss: 0.0083, Train Acc: 0.8611, Test Acc: 0.8870, Train Detached Loss: 0.0071, Test Detached Loss: 0.0071, Learning Rate: 0.000001, Epoch Time: 1.54 seconds\n",
      "Epoch 390/600: Train Loss: 0.0079, Train Acc: 0.8664, Test Acc: 0.8930, Train Detached Loss: 0.0068, Test Detached Loss: 0.0067, Learning Rate: 0.000001, Epoch Time: 1.50 seconds\n",
      "Epoch 400/600: Train Loss: 0.0075, Train Acc: 0.8736, Test Acc: 0.9000, Train Detached Loss: 0.0064, Test Detached Loss: 0.0064, Learning Rate: 0.000001, Epoch Time: 1.51 seconds\n",
      "Epoch 410/600: Train Loss: 0.0072, Train Acc: 0.8802, Test Acc: 0.9025, Train Detached Loss: 0.0061, Test Detached Loss: 0.0061, Learning Rate: 0.000001, Epoch Time: 1.53 seconds\n",
      "Epoch 420/600: Train Loss: 0.0069, Train Acc: 0.8854, Test Acc: 0.9068, Train Detached Loss: 0.0059, Test Detached Loss: 0.0058, Learning Rate: 0.000001, Epoch Time: 1.50 seconds\n",
      "Epoch 430/600: Train Loss: 0.0066, Train Acc: 0.8896, Test Acc: 0.9118, Train Detached Loss: 0.0056, Test Detached Loss: 0.0056, Learning Rate: 0.000001, Epoch Time: 1.60 seconds\n",
      "Epoch 440/600: Train Loss: 0.0064, Train Acc: 0.8941, Test Acc: 0.9152, Train Detached Loss: 0.0054, Test Detached Loss: 0.0054, Learning Rate: 0.000001, Epoch Time: 1.80 seconds\n",
      "Epoch 450/600: Train Loss: 0.0062, Train Acc: 0.8965, Test Acc: 0.9174, Train Detached Loss: 0.0052, Test Detached Loss: 0.0052, Learning Rate: 0.000001, Epoch Time: 1.51 seconds\n",
      "Epoch 460/600: Train Loss: 0.0059, Train Acc: 0.9008, Test Acc: 0.9196, Train Detached Loss: 0.0050, Test Detached Loss: 0.0050, Learning Rate: 0.000001, Epoch Time: 1.53 seconds\n",
      "Epoch 470/600: Train Loss: 0.0058, Train Acc: 0.9029, Test Acc: 0.9221, Train Detached Loss: 0.0048, Test Detached Loss: 0.0049, Learning Rate: 0.000001, Epoch Time: 1.52 seconds\n",
      "Epoch 480/600: Train Loss: 0.0055, Train Acc: 0.9067, Test Acc: 0.9234, Train Detached Loss: 0.0047, Test Detached Loss: 0.0047, Learning Rate: 0.000001, Epoch Time: 1.53 seconds\n",
      "Epoch 490/600: Train Loss: 0.0054, Train Acc: 0.9095, Test Acc: 0.9252, Train Detached Loss: 0.0045, Test Detached Loss: 0.0046, Learning Rate: 0.000001, Epoch Time: 1.52 seconds\n",
      "Epoch 500/600: Train Loss: 0.0052, Train Acc: 0.9125, Test Acc: 0.9273, Train Detached Loss: 0.0044, Test Detached Loss: 0.0045, Learning Rate: 0.000001, Epoch Time: 1.51 seconds\n",
      "Epoch 510/600: Train Loss: 0.0051, Train Acc: 0.9132, Test Acc: 0.9291, Train Detached Loss: 0.0043, Test Detached Loss: 0.0044, Learning Rate: 0.000001, Epoch Time: 1.75 seconds\n",
      "Epoch 520/600: Train Loss: 0.0050, Train Acc: 0.9167, Test Acc: 0.9317, Train Detached Loss: 0.0042, Test Detached Loss: 0.0043, Learning Rate: 0.000001, Epoch Time: 1.51 seconds\n",
      "Epoch 530/600: Train Loss: 0.0048, Train Acc: 0.9190, Test Acc: 0.9331, Train Detached Loss: 0.0040, Test Detached Loss: 0.0042, Learning Rate: 0.000001, Epoch Time: 1.51 seconds\n",
      "Epoch 540/600: Train Loss: 0.0047, Train Acc: 0.9206, Test Acc: 0.9344, Train Detached Loss: 0.0040, Test Detached Loss: 0.0041, Learning Rate: 0.000001, Epoch Time: 1.52 seconds\n",
      "Epoch 550/600: Train Loss: 0.0046, Train Acc: 0.9215, Test Acc: 0.9356, Train Detached Loss: 0.0039, Test Detached Loss: 0.0040, Learning Rate: 0.000001, Epoch Time: 1.56 seconds\n",
      "Epoch 560/600: Train Loss: 0.0045, Train Acc: 0.9245, Test Acc: 0.9361, Train Detached Loss: 0.0038, Test Detached Loss: 0.0039, Learning Rate: 0.000001, Epoch Time: 1.53 seconds\n",
      "Epoch 570/600: Train Loss: 0.0044, Train Acc: 0.9248, Test Acc: 0.9371, Train Detached Loss: 0.0037, Test Detached Loss: 0.0038, Learning Rate: 0.000001, Epoch Time: 1.55 seconds\n",
      "Epoch 580/600: Train Loss: 0.0043, Train Acc: 0.9277, Test Acc: 0.9383, Train Detached Loss: 0.0036, Test Detached Loss: 0.0037, Learning Rate: 0.000001, Epoch Time: 1.52 seconds\n",
      "Epoch 590/600: Train Loss: 0.0042, Train Acc: 0.9298, Test Acc: 0.9390, Train Detached Loss: 0.0035, Test Detached Loss: 0.0037, Learning Rate: 0.000001, Epoch Time: 1.54 seconds\n",
      "Epoch 600/600: Train Loss: 0.0041, Train Acc: 0.9305, Test Acc: 0.9398, Train Detached Loss: 0.0034, Test Detached Loss: 0.0037, Learning Rate: 0.000001, Epoch Time: 1.52 seconds\n",
      "     epoch  train_loss  train_acc  test_acc  train_detached_loss  \\\n",
      "0        1    0.046782   0.098917    0.0997             0.042535   \n",
      "1        2    0.046251   0.098133    0.1001             0.041759   \n",
      "2        3    0.045454   0.099067    0.0974             0.046604   \n",
      "3        4    0.045968   0.100033    0.0958             0.049534   \n",
      "4        5    0.045311   0.099083    0.0958             0.045196   \n",
      "..     ...         ...        ...       ...                  ...   \n",
      "595    596    0.004132   0.928833    0.9398             0.003438   \n",
      "596    597    0.004203   0.929000    0.9398             0.003429   \n",
      "597    598    0.004126   0.930817    0.9406             0.003420   \n",
      "598    599    0.004149   0.928750    0.9407             0.003416   \n",
      "599    600    0.004113   0.930500    0.9398             0.003414   \n",
      "\n",
      "     test_detached_loss optimizer loss_fn repetition  initial_lr  actual_lr  \\\n",
      "0              0.042941      Adam    None       None    0.000001   0.000001   \n",
      "1              0.042115      Adam    None       None    0.000001   0.000001   \n",
      "2              0.046959      Adam    None       None    0.000001   0.000001   \n",
      "3              0.050008      Adam    None       None    0.000001   0.000001   \n",
      "4              0.045474      Adam    None       None    0.000001   0.000001   \n",
      "..                  ...       ...     ...        ...         ...        ...   \n",
      "595            0.003618      Adam    None       None    0.000001   0.000001   \n",
      "596            0.003619      Adam    None       None    0.000001   0.000001   \n",
      "597            0.003605      Adam    None       None    0.000001   0.000001   \n",
      "598            0.003598      Adam    None       None    0.000001   0.000001   \n",
      "599            0.003699      Adam    None       None    0.000001   0.000001   \n",
      "\n",
      "     corr_p  epoch_time  \n",
      "0       0.0    1.704602  \n",
      "1       0.0    1.515076  \n",
      "2       0.0    1.515486  \n",
      "3       0.0    1.530724  \n",
      "4       0.0    1.554518  \n",
      "..      ...         ...  \n",
      "595     0.0    1.601617  \n",
      "596     0.0    1.635880  \n",
      "597     0.0    1.584229  \n",
      "598     0.0    1.525462  \n",
      "599     0.0    1.524939  \n",
      "\n",
      "[600 rows x 13 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "\"\"\" import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "def resnet18_cifar(num_classes: int):\n",
    "    m = models.resnet18(weights=None)\n",
    "    # CIFAR10: 32x32，改第一层卷积 + 去掉 maxpool\n",
    "    m.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    m.maxpool = nn.Identity()\n",
    "    m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
    "    return m\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = resnet18_cifar(Data.num_classes).to(device)\n",
    " \"\"\"\n",
    "\n",
    "\"\"\" optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.1,\n",
    "    momentum=0.9,\n",
    "    weight_decay=5e-4\n",
    ") \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 0) 先从 loader 拿一个 batch 推断 input_dim\n",
    "\"\"\" xb, wb, yb = next(iter(train_loader))\n",
    "input_dim = int(np.prod(xb.shape[1:]))   # CIFAR10 通常是 3*32*32=3072\n",
    "\n",
    "print(\"xb shape:\", xb.shape, \"=> input_dim:\", input_dim)\n",
    "\n",
    "# 1) 模型：先 Flatten 再 MLP（这样即使 xb 是 4D 也能喂给 MLP）\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(start_dim=1),\n",
    "    MLP(\n",
    "        input_size=input_dim,\n",
    "        hidden_sizes=[],\n",
    "        output_size=Data.num_classes,\n",
    "        dropout_p=0,\n",
    "        bn=False,\n",
    "        activation='relu'\n",
    "    )\n",
    ")\n",
    " \"\"\"\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[256, 128],    #MLP两层隐藏层\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0.2,       #加一点dropout\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "\n",
    "#optimizer = optim.SGD(\n",
    "#    model.parameters(),\n",
    "#    lr=0.01,\n",
    "#    momentum=0.0  # 0.9\n",
    "#)\n",
    "\n",
    "# 2. Training parameters\n",
    "num_epochs = 600\n",
    "\n",
    "em_loss = MarginalChainProperLoss(weakener.M, loss_code=\"cross_entropy\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47ad3c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getenv(\"PYTHONBREAKPOINT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a89db1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Initializing MLP model...\n",
      "784\n",
      "[]\n",
      "10\n",
      "Epoch 10/60: Train Loss: 0.0045, Train Acc: 0.9193, Test Acc: 0.9209, Train Detached Loss: 0.0045, Test Detached Loss: 0.0045, Learning Rate: 0.000001, Epoch Time: 1.14 seconds\n",
      "Epoch 20/60: Train Loss: 0.0042, Train Acc: 0.9253, Test Acc: 0.9257, Train Detached Loss: 0.0042, Test Detached Loss: 0.0042, Learning Rate: 0.000001, Epoch Time: 0.94 seconds\n",
      "Epoch 30/60: Train Loss: 0.0040, Train Acc: 0.9280, Test Acc: 0.9265, Train Detached Loss: 0.0040, Test Detached Loss: 0.0042, Learning Rate: 0.000001, Epoch Time: 0.95 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     23\u001b[39m em_loss = ForwardProperLoss(weakener.M, loss_code=\u001b[33m\"\u001b[39m\u001b[33mcross_entropy\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m#em_loss = MarginalChainProperLoss(weakener.M, loss_code=\"cross_entropy\")\u001b[39;00m\n\u001b[32m     25\u001b[39m \n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# 3. Run the training + evaluation loop\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m model, results_df = \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# our MLP on device\u001b[39;49;00m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# yields (x, w, y)\u001b[39;49;00m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# yields (x, y)\u001b[39;49;00m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Adam optimizer\u001b[39;49;00m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mem_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# EMLoss with our PLL mixing matrix\u001b[39;49;00m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# total epochs\u001b[39;49;00m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcorr_p\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# used for logging consistency\u001b[39;49;00m\n\u001b[32m     35\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# 4. View the epoch‐by‐epoch results\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(results_df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Weak_MC-1/utils/train_test_loop.py:51\u001b[39m, in \u001b[36mtrain_and_evaluate\u001b[39m\u001b[34m(model, trainloader, testloader, optimizer, loss_fn, num_epochs, corr_p, rep, sound, loss_type, clothing)\u001b[39m\n\u001b[32m     48\u001b[39m running_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m     49\u001b[39m correct_train = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     52\u001b[39m \n\u001b[32m     53\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;66;43;03m#    if i == 0:\u001b[39;49;00m\n\u001b[32m     54\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;66;43;03m#       for name, param in model.named_parameters():\u001b[39;49;00m\n\u001b[32m     55\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;66;43;03m#          print(name, param)\u001b[39;49;00m\n\u001b[32m     56\u001b[39m \n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#if loss_type == 'Supervised':\u001b[39;49;00m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#    train_targets = torch.max(targets, dim=1)[1]\u001b[39;49;00m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Weak_MC/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Weak_MC/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Weak_MC/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Weak_MC/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:398\u001b[39m, in \u001b[36mdefault_collate\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_collate\u001b[39m(batch):\n\u001b[32m    338\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[33;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[32m    340\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m \u001b[33;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Weak_MC/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:212\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    208\u001b[39m transposed = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(*batch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[32m    214\u001b[39m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Weak_MC/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:155\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Weak_MC/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:272\u001b[39m, in \u001b[36mcollate_tensor_fn\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    270\u001b[39m     storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n\u001b[32m    271\u001b[39m     out = elem.new(storage).resize_(\u001b[38;5;28mlen\u001b[39m(batch), *\u001b[38;5;28mlist\u001b[39m(elem.size()))\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "\n",
    "\n",
    "# 2. Training parameters\n",
    "num_epochs = 60\n",
    "\n",
    "#em_loss = FwdLoss(weakener.M)\n",
    "em_loss = ForwardProperLoss(weakener.M, loss_code=\"cross_entropy\")\n",
    "#em_loss = MarginalChainProperLoss(weakener.M, loss_code=\"cross_entropy\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910d1634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 1. 固定随机种子\n",
    "torch.manual_seed(0)\n",
    "\n",
    "B, C = 5, 4\n",
    "\n",
    "logits = torch.randn(B, C, requires_grad=True)\n",
    "z = torch.randint(0, C, (B,))\n",
    "\n",
    "M = torch.rand(C, C)\n",
    "M = M / M.sum(dim=1, keepdim=True)\n",
    "F = M.clone()\n",
    "\n",
    "# 这里贴上你的 MarginalChainProperLoss 和 ForwardProperLoss 定义\n",
    "# loss_code=\"cross_entropy\"\n",
    "\n",
    "mc_loss_fn = MarginalChainProperLoss(M, loss_code=\"cross_entropy\", reduction=\"mean\")\n",
    "fw_loss_fn = ForwardProperLoss(F, loss_code=\"cross_entropy\", reduction=\"mean\")\n",
    "\n",
    "# Marginal chain\n",
    "logits_mc = logits.clone().detach().requires_grad_(True)\n",
    "loss_mc = mc_loss_fn(logits_mc, z)\n",
    "loss_mc.backward()\n",
    "grad_mc = logits_mc.grad.clone().detach()\n",
    "\n",
    "# Forward\n",
    "logits_fw = logits.clone().detach().requires_grad_(True)\n",
    "loss_fw = fw_loss_fn(logits_fw, z)\n",
    "loss_fw.backward()\n",
    "grad_fw = logits_fw.grad.clone().detach()\n",
    "\n",
    "print(\"loss_mc:\", loss_mc.item())\n",
    "print(\"loss_fw:\", loss_fw.item())\n",
    "print(\"loss diff:\", abs(loss_mc.item() - loss_fw.item()))\n",
    "\n",
    "print(\"grad same?\", torch.allclose(grad_mc, grad_fw, atol=1e-6))\n",
    "print(\"grad max diff:\", (grad_mc - grad_fw).abs().max().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c50f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取一个 batch\n",
    "xb, zb, yb = next(iter(train_loader))   # 确保 zb 就是 z（weak index）\n",
    "xb = xb.to(device)\n",
    "zb = zb.to(device)\n",
    "\n",
    "logits = model(xb)\n",
    "\n",
    "fwd_loss_fn = ForwardProperLoss(weakener.M, \"cross_entropy\").to(device)\n",
    "ub_loss_fn  = UpperBoundWeakProperLoss(weakener.M, \"cross_entropy\").to(device)\n",
    "\n",
    "loss_fwd = fwd_loss_fn(logits, zb)\n",
    "loss_ub  = ub_loss_fn(logits, zb)\n",
    "\n",
    "print(\"loss_fwd:\", loss_fwd.item())\n",
    "print(\"loss_ub :\", loss_ub.item())\n",
    "\n",
    "g1 = torch.autograd.grad(loss_fwd, logits, retain_graph=True)[0]\n",
    "g2 = torch.autograd.grad(loss_ub,  logits)[0]\n",
    "print(\"grad norm fwd:\", g1.norm().item())\n",
    "print(\"grad norm ub :\",  g2.norm().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0822459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "\n",
    "\n",
    "# 2. Training parameters\n",
    "num_epochs = 60\n",
    "\n",
    "#em_loss = FwdLoss(weakener.M)\n",
    "em_loss = UpperBoundWeakProperLoss(weakener.M, loss_code=\"cross_entropy\")\n",
    "#em_loss = MarginalChainProperLoss(weakener.M, loss_code=\"cross_entropy\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d20a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "# 2. Training parameters\n",
    "num_epochs = 60\n",
    "\n",
    "em_loss = MarginalChainProperLoss(weakener.M, loss_code=\"ps_2\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31519ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.015,\n",
    ")\n",
    "# 2. Training parameters\n",
    "num_epochs = 90\n",
    "\n",
    "em_loss = ForwardProperLoss(weakener.M, loss_code=\"ps_2\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d1d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "# 2. Training parameters\n",
    "num_epochs = 60\n",
    "\n",
    "em_loss = UpperBoundWeakProperLoss(weakener.M, loss_code=\"ps_2\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34351956",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.015,\n",
    ")\n",
    "# 2. Training parameters\n",
    "num_epochs = 400\n",
    "\n",
    "em_loss = MarginalChainProperLoss(weakener.M, loss_code=\"tsallis_0.2\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f39e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "# 2. Training parameters\n",
    "num_epochs = 60\n",
    "\n",
    "em_loss = ForwardProperLoss(weakener.M, loss_code=\"tsallis_0.5\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3583bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "# 2. Training parameters\n",
    "num_epochs = 60\n",
    "\n",
    "em_loss = UpperBoundWeakProperLoss(weakener.M, loss_code=\"tsallis_0.2\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3350bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Weak Train Loss')\n",
    "ax1.plot(clean_results['epoch'], clean_results['train_loss'], label='Supervised Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Weak Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Weak Test Accuracy')\n",
    "ax2.plot(clean_results['epoch'], clean_results['train_acc'],'--', label='Supervised Train Accuracy' )\n",
    "ax2.plot(clean_results['epoch'], clean_results['test_acc'], '--', label='Supervied Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfceed9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
